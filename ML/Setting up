Following are the steps:
1. Understand the problem, this includes asking 'How will the output data produced by Machine Learning Algorithm be used'? This helps in deciding the Machine Learning Algorithm to use, the format of the data to be collected etc. Consider for example the problem of predicting House Prices, the prediction can be done in two ways, either we can predict an absolute price or catagories the price of house as either low, mid or high ranged. The two cases will require different Supervised Learning Algorithms(Regression and Classification respectively) and also different format of data to train the model.
2. Gather the data, it is advisable to write a Python script(we are coding in Python hence a Python script as the code can be integrated with our main code) that fetches data as we can schedule the execution of it and fecth fresh data on regular basis. One such script is written below, this script can be tweaked a bit and made to work for any other data set download:

import os
import tarfile
from six.moves import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/" # download website
HOUSING_PATH = os.path.join("datasets", "housing") # download path in current directory
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz" # download file in download website
def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
	if not os.path.isdir(housing_path):
		os.makedirs(housing_path)
	tgz_path = os.path.join(housing_path, "housing.tgz")
	urllib.request.urlretrieve(housing_url, tgz_path)
	housing_tgz = tarfile.open(tgz_path) # opening the tar file, again it may need to be changed if file to be downloaded is not in tar
	housing_tgz.extractall(path=housing_path)
	housing_tgz.close()


3. Reading the data. Data can come as some separted variable format(mostly csv). This can be read using read_csv method of pandas, read_csv allows to provide an optional argumant 'deliminator' which can be used to sepcify the deliminator if data is not separated by comma. read_csv returns DataFrame object and hence we can apply operations of DataFrame on the returned value. Again one example of a function that returns DataFrame is given below, tweak it as per requirement:

def load_housing_data(housing_path=HOUSING_PATH):
	csv_path = os.path.join(housing_path, "housing.csv")
	return pd.read_csv(csv_path)

3. Visualise the data. We can use the plotting function provided in matplotlib for this purpose. 


4. Prepare Training and Test sets. From the fetched data we now need to prepare test and training set, first we need to decide the percentage of data to be taken for test set. Creating a test set is theoretically quite simple: just pick some instances randomly, typically 20% of the
dataset, and set them aside:

import numpy as np
def split_train_test(data, test_ratio):
	shuffled_indices = np.random.permutation(len(data))
	test_set_size = int(len(data) * test_ratio)
	test_indices = shuffled_indices[:test_set_size]
	train_indices = shuffled_indices[test_set_size:]
	return data.iloc[train_indices], data.iloc[test_indices]
